import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from typing import Optional

# è·å–é¡¹ç›®æ ¹ç›®å½•è·¯å¾„ (ç°åœ¨æ˜¯ backend çš„ä¸Šçº§ç›®å½•)
current_file_path = os.path.abspath(__file__)  # .../backend/app/core/llms.py
app_dir = os.path.dirname(current_file_path)  # .../backend/app/core
backend_app_dir = os.path.dirname(app_dir)  # .../backend/app
backend_dir = os.path.dirname(backend_app_dir)  # .../backend
project_root = os.path.dirname(backend_dir)  # .../ (é¡¹ç›®æ ¹ç›®å½•)
env_path = os.path.join(project_root, ".env")

print(f"å½“å‰æ–‡ä»¶è·¯å¾„: {current_file_path}")
print(f"app ç›®å½•: {app_dir}")
print(f"backend/app ç›®å½•: {backend_app_dir}")
print(f"backend ç›®å½•: {backend_dir}")
print(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
print(f"ç¯å¢ƒæ–‡ä»¶è·¯å¾„: {env_path}")

# å°è¯•åŠ è½½ç¯å¢ƒå˜é‡
if not load_dotenv(env_path):
    print(f"è­¦å‘Š: æ— æ³•ä» {env_path} åŠ è½½ç¯å¢ƒå˜é‡")
    print("å½“å‰å·¥ä½œç›®å½•:", os.getcwd())
else:
    print("âœ… ç¯å¢ƒå˜é‡åŠ è½½æˆåŠŸ")

# ============================================================================
# LLM åŒé€šé“é…ç½®
# ============================================================================

class LLMChannel:
    """LLMé€šé“é…ç½®ç±»"""
    
    def __init__(self, api_key: str, base_url: str, model_name: str, temperature: float = 0.7):
        self.api_key = api_key
        self.base_url = base_url
        self.model_name = model_name
        self.temperature = temperature
        self._llm_instance: Optional[ChatOpenAI] = None
    
    def get_llm(self) -> ChatOpenAI:
        """è·å–LLMå®ä¾‹ï¼ˆæ‡’åŠ è½½ï¼‰"""
        if self._llm_instance is None:
            self._llm_instance = ChatOpenAI(
                temperature=self.temperature,
                max_tokens=8000,
                model_name=self.model_name,
                api_key=self.api_key,
                base_url=self.base_url
            )
        return self._llm_instance

class LLMFactory:
    """LLMå·¥å‚ç±»ï¼Œç®¡ç†åŒé€šé“é…ç½®"""
    
    def __init__(self):
        self._fast_channel: Optional[LLMChannel] = None
        self._smart_channel: Optional[LLMChannel] = None
        self._legacy_llm: Optional[ChatOpenAI] = None
        self._init_channels()
    
    def _init_channels(self):
        """åˆå§‹åŒ–åŒé€šé“é…ç½®"""
        # Fast Channel - ç”¨äºæ„å›¾è¯†åˆ«ã€ç®€å•åˆ†ç±»
        fast_api_key = os.getenv("FAST_LLM_API_KEY")
        fast_base_url = os.getenv("FAST_LLM_BASE_URL")
        fast_model = os.getenv("FAST_LLM_MODEL")
        
        if fast_api_key and fast_base_url and fast_model:
            self._fast_channel = LLMChannel(
                api_key=fast_api_key,
                base_url=fast_base_url,
                model_name=fast_model,
                temperature=0.7  
            )
            print("âœ… Fast Channel åˆå§‹åŒ–æˆåŠŸ")
        else:
            print("âš ï¸ Fast Channel é…ç½®ä¸å®Œæ•´ï¼Œå°†ä½¿ç”¨é»˜è®¤LLM")
        
        # Smart Channel - ç”¨äºç”Ÿæˆå¤æ‚çš„é¢è¯•å®˜å›å¤ã€ç‚¹è¯„å’Œæ€»ç»“
        smart_api_key = os.getenv("SMART_LLM_API_KEY")
        smart_base_url = os.getenv("SMART_LLM_BASE_URL")
        smart_model = os.getenv("SMART_LLM_MODEL")
        
        if smart_api_key and smart_base_url and smart_model:
            self._smart_channel = LLMChannel(
                api_key=smart_api_key,
                base_url=smart_base_url,
                model_name=smart_model,
                temperature=0.7  # å…è®¸ä¸€å®šçš„åˆ›é€ æ€§
            )
            print("âœ… Smart Channel åˆå§‹åŒ–æˆåŠŸ")
        else:
            print("âš ï¸ Smart Channel é…ç½®ä¸å®Œæ•´ï¼Œå°†ä½¿ç”¨é»˜è®¤LLM")
        
        # åˆå§‹åŒ–ä¼ ç»ŸLLMä½œä¸ºåå¤‡
        self._init_legacy_llm()
    
    def _init_legacy_llm(self):
        """åˆå§‹åŒ–ä¼ ç»ŸLLMä½œä¸ºåå¤‡"""
        # éªŒè¯ç¯å¢ƒå˜é‡æ˜¯å¦åŠ è½½æˆåŠŸ
        api_model = os.getenv("XINLIU_API_MODEL")
        if not api_model:
            print("é”™è¯¯: XINLIU_API_MODEL ç¯å¢ƒå˜é‡æœªè®¾ç½®")
            print("å¯ç”¨çš„ç¯å¢ƒå˜é‡:")
            for key, value in os.environ.items():
                if 'API' in key.upper() or 'XINLIU' in key.upper():
                    print(f"  {key}: {value}")
            raise ValueError("å¿…éœ€çš„ç¯å¢ƒå˜é‡ XINLIU_API_MODEL æœªè®¾ç½®")
        
        self._legacy_llm = ChatOpenAI(
            temperature=0.7,
            max_tokens=8000,
            model_name=os.getenv("XINLIU_API_MODEL"),
            api_key=os.getenv("XINLIU_API_KEY"),
            base_url=os.getenv("XINLIU_API_BASE")
        )
        print("âœ… Legacy LLM åˆå§‹åŒ–æˆåŠŸ")
    
    def get_fast_llm(self) -> ChatOpenAI:
        """è·å–Fast Channelçš„LLMå®ä¾‹"""
        if self._fast_channel:
            return self._fast_channel.get_llm()
        return self._legacy_llm
    
    def get_smart_llm(self) -> ChatOpenAI:
        """è·å–Smart Channelçš„LLMå®ä¾‹"""
        if self._smart_channel:
            return self._smart_channel.get_llm()
        return self._legacy_llm
    
    def get_legacy_llm(self) -> ChatOpenAI:
        """è·å–ä¼ ç»ŸLLMå®ä¾‹ï¼ˆå‘åå…¼å®¹ï¼‰"""
        return self._legacy_llm

# å…¨å±€å·¥å‚å®ä¾‹
_llm_factory = LLMFactory()

# ============================================================================
# å‘åå…¼å®¹çš„æ¥å£
# ============================================================================

def get_llm():
    """è·å–é»˜è®¤LLMå®ä¾‹ï¼ˆå‘åå…¼å®¹ï¼‰"""
    return _llm_factory.get_legacy_llm()

def get_fast_llm():
    """è·å–Fast Channel LLMå®ä¾‹"""
    return _llm_factory.get_fast_llm()

def get_smart_llm():
    """è·å–Smart Channel LLMå®ä¾‹"""
    return _llm_factory.get_smart_llm()


# ============================================================================
# åŠ¨æ€ LLM åˆ›å»ºï¼ˆæ”¯æŒç”¨æˆ·è‡ªå®šä¹‰é…ç½®ï¼‰
# ============================================================================

def create_llm_from_config(
    api_key: str,
    base_url: str,
    model: str,
    temperature: float = 0.7,
    max_tokens: int = 8000
) -> ChatOpenAI:
    """
    æ ¹æ®ç”¨æˆ·æä¾›çš„é…ç½®åˆ›å»º LLM å®ä¾‹
    
    Args:
        api_key: API Key
        base_url: API Base URL
        model: æ¨¡å‹åç§°
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§ token æ•°
        
    Returns:
        ChatOpenAI: LLM å®ä¾‹
    """
    return ChatOpenAI(
        temperature=temperature,
        max_tokens=max_tokens,
        model_name=model,
        api_key=api_key,
        base_url=base_url
    )


def get_llm_for_request(api_config: Optional[dict] = None, channel: str = "smart") -> ChatOpenAI:
    """
    è·å–ç”¨äºå¤„ç†è¯·æ±‚çš„ LLM å®ä¾‹
    
    ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨æœåŠ¡å™¨é»˜è®¤é…ç½®
    æ”¯æŒåŒé€šé“ï¼šsmart_model ç”¨äºå¤æ‚ä»»åŠ¡ï¼Œfast_model ç”¨äºå¿«é€Ÿå“åº”
    
    Args:
        api_config: ç”¨æˆ·çš„ API é…ç½®ï¼ŒåŒ…å« api_key, base_url, smart_model, fast_model
        channel: ä½¿ç”¨çš„é€šé“ï¼Œ"fast" æˆ– "smart"
        
    Returns:
        ChatOpenAI: LLM å®ä¾‹
    """
    # å¦‚æœç”¨æˆ·æä¾›äº†é…ç½®ï¼Œä½¿ç”¨ç”¨æˆ·é…ç½®
    if api_config and api_config.get("api_key"):
        # æ ¹æ® channel é€‰æ‹©å¯¹åº”çš„æ¨¡å‹
        model = api_config.get("smart_model") if channel == "smart" else api_config.get("fast_model")
        print(f"ğŸ“± ä½¿ç”¨ç”¨æˆ·è‡ªå®šä¹‰ API é…ç½® ({channel}): {model}")
        return create_llm_from_config(
            api_key=api_config["api_key"],
            base_url=api_config["base_url"],
            model=model
        )
    
    # å¦åˆ™ä½¿ç”¨æœåŠ¡å™¨é»˜è®¤é…ç½®
    print(f"ğŸ–¥ï¸ ä½¿ç”¨æœåŠ¡å™¨é»˜è®¤ API é…ç½® ({channel} channel)")
    if channel == "fast":
        return get_fast_llm()
    else:
        return get_smart_llm()


# ============================================================================
# æµ‹è¯•ä»£ç 
# ============================================================================

if __name__ == "__main__":
    import asyncio
    
    async def main():
        try:
            # æµ‹è¯•ä¼ ç»ŸLLM
            legacy_llm = get_llm()
            response = await legacy_llm.ainvoke("ä½ å¥½ï¼Œè¯·å›å¤ï¼šLegacy LLMé…ç½®æˆåŠŸï¼")
            print("Legacy LLMå“åº”:", response.content)
            
            # æµ‹è¯•Fast Channel
            fast_llm = get_fast_llm()
            response = await fast_llm.ainvoke("ä½ å¥½ï¼Œè¯·å›å¤ï¼šFast Channelé…ç½®æˆåŠŸï¼")
            print("Fast Channelå“åº”:", response.content)
            
            # æµ‹è¯•Smart Channel
            smart_llm = get_smart_llm()
            response = await smart_llm.ainvoke("ä½ å¥½ï¼Œè¯·å›å¤ï¼šSmart Channelé…ç½®æˆåŠŸï¼")
            print("Smart Channelå“åº”:", response.content)
            
        except Exception as e:
            print(f"LLM é…ç½®å¤±è´¥: {e}")

    asyncio.run(main())
